{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOECqN7psPGQ+sqT1DRKUCo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEmELl0KP178","executionInfo":{"status":"ok","timestamp":1636205084260,"user_tz":-480,"elapsed":25679,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"2f2e93be-0e11-4fb0-9d4c-4fed18764e92"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","os.chdir('/content/gdrive/MyDrive/iss/PRS/PM/')\n","!ls\n","\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/iss/PRS/PM/vision/references/detection')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"," archive\t\t\t       maskrcnn-benchmark-main\n"," checkpoin\t\t\t       mask_rcnn_coco.h5\n"," checkpoint\t\t\t       my_data\n"," COVID-CT-Mask-Net-master\t       my_data2\n"," ct_lesion_seg.zip\t\t       __pycache__\n","'ct_lesion_seg.zip (Unzipped Files)'   Untitled0.ipynb\n"," ct_seg_model.pth\t\t       Untitled1.ipynb\n"," data\t\t\t\t      'Untitled2(dont need).ipynb'\n"," lesion_mask_data_process.ipynb        Untitled2.ipynb\n"," lung_mask_data\t\t\t       Untitled3.ipynb\n"," lung_mask_model.pth\t\t       Untitled4.ipynb\n"," lung_mask_model_train.ipynb\t       Untitled5.ipynb\n"," marchine_learning.ipynb\t       vision\n"," MASK_RCNN\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtwTYJLdP_CL","executionInfo":{"status":"ok","timestamp":1636205120247,"user_tz":-480,"elapsed":14075,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"5740554b-621e-4951-fe5b-7b2dbec1fd4d"},"source":["import torch\n","print(\"PyTorch version is\", torch.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version is 1.9.0+cu111\n"]}]},{"cell_type":"code","metadata":{"id":"Vf30KbeTaT3H"},"source":["# import cv2\n","# from google.colab.patches import cv2_imshow\n","\n","# im = cv2.imread('ct_lesion_seg/mask/8/28.png')\n","# obj_ids = np.unique(im)\n","\n","# for k in range(3):\n","#   for i in range(im.shape[0]):\n","#     for j in range(im.shape[1]):\n","#       if(im[i][j][k] == 1):\n","#         im[i][j][k] = 255\n","\n","# cv2_imshow(im)\n","\n","# im2 = cv2.imread('ct_lesion_seg/image/8/28.jpg')\n","# cv2_imshow(im2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYA4JxX_egPc"},"source":["# # Jeremy: some masks dont have lesion, remove them\n","\n","# counter = 0\n","# mask_list = []\n","# folder_list = []\n","# folder_list = list(sorted(os.listdir(os.path.join('ct_lesion_seg', \"mask\"))))\n","# for i in folder_list:\n","#   mask_list.append(list(sorted(os.listdir(os.path.join('ct_lesion_seg', \"mask\", i)))))\n","\n","# for i in range(150):\n","#   for j in mask_list[i]:\n","#     mask_path = os.path.join('ct_lesion_seg', \"mask\", folder_list[i], j)\n","#     mask = Image.open(mask_path)\n","#     mask = np.array(mask)\n","#     obj_ids = np.unique(mask)[2:]\n","#     if(len(obj_ids) == 0):\n","#       os.remove(mask_path)\n","#     else:\n","#       counter = counter + 1\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"KV5wYgH-6Qke","executionInfo":{"status":"error","timestamp":1635736874597,"user_tz":-480,"elapsed":5135,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"15961319-affa-4ed3-a26c-8b837387716f"},"source":["from skimage.measure import label as method_label\n","from skimage.measure import regionprops\n","import numpy as np\n","\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","im = cv2.imread('ct_lesion_seg/mask/0/46.png')\n","obj_ids = np.unique(im)\n","\n","obj_ids = obj_ids[2:]\n","\n","# for k in range(3):\n","#   for i in range(im.shape[0]):\n","#     for j in range(im.shape[1]):\n","#       if(im[i][j][k] == 1):\n","#         im[i][j][k] = 255\n","#       elif(im[i][j][k] == 2):\n","#         im[i][j][k] = 50\n","#       elif(im[i][j][k] == 3):\n","#         im[i][j][k] = 150\n","\n","# cv2_imshow(im)\n","\n","# im2 = cv2.imread('ct_lesion_seg/image/0/46.jpg')\n","# cv2_imshow(im2)\n","\n","masks = im == obj_ids[:, None, None]\n","\n","print(masks.shape)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0cd344009376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ct_lesion_seg/mask/0/46.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mobj_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"QXCkdUgAQAqi"},"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","from skimage.measure import label as method_label\n","from skimage.measure import regionprops\n","\n","class Ct_seg_dataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        self.folders = list(sorted(os.listdir(os.path.join(self.root, \"image\"))))\n","        self.masks = []\n","\n","        for i in self.folders:\n","          self.masks.append(list(sorted(os.listdir(os.path.join(self.root, \"mask\", i)))))\n","        \n","        self.idx_list = []\n","        for i in range(len(self.masks)):\n","          for j in range(len(self.masks[i])):\n","            self.idx_list.append([i, j])\n","    \n","    def extract_single_mask(self, mask, lab):\n","        _mask = np.zeros(mask.shape, dtype=np.uint8)\n","        area = mask == lab\n","        _mask[area] = 1\n","        return _mask\n","        \n","\n","    def __getitem__(self, idx):\n","        # load images and masks\n","        mask_i = self.idx_list[idx][0]\n","        mask_j = self.idx_list[idx][1]\n","        # Jeremy: change 'xxx.png' to 'xxx.jpg'\n","        img_name = \"\".join(list(filter(str.isdigit, self.masks[mask_i][mask_j])))+'.jpg'\n","        img_path = os.path.join(self.root, \"image\", self.folders[mask_i], img_name)\n","        mask_path = os.path.join(self.root, \"mask\", self.folders[mask_i], self.masks[mask_i][mask_j])\n","        img = Image.open(img_path).convert(\"RGB\")\n","        # note that we haven't converted the mask to RGB,\n","        # because each color corresponds to a different instance\n","        # with 0 being background\n","        mask = Image.open(mask_path)\n","        # convert the PIL Image into a numpy array\n","        mask = np.array(mask)\n","        # instances are encoded as different colors\n","        obj_ids = np.unique(mask)\n","        # first id is the background, so remove it\n","        # Jeremy: also remove second id becaues it is the lung shape in our dataset\n","        obj_ids = obj_ids[2:]\n","\n","        # split the color-encoded mask into a set\n","        # of binary masks\n","        mask_classes = mask == obj_ids[:, None, None]\n","\n","        # # get bounding box coordinates for each mask\n","        # num_objs = len(obj_ids)\n","        # boxes = []\n","        # for i in range(num_objs):\n","        #     pos = np.where(masks[i])\n","        #     xmin = np.min(pos[1])\n","        #     xmax = np.max(pos[1])\n","        #     ymin = np.min(pos[0])\n","        #     ymax = np.max(pos[0])\n","        #     boxes.append([xmin, ymin, xmax, ymax])\n","\n","        boxes = []\n","        masks = []\n","        labels = []\n","        for _idx, m in enumerate(mask_classes):\n","            lab_mask = method_label(m)\n","            regions = regionprops(lab_mask)\n","            for _i, r in enumerate(regions):\n","                # get rid of really small ones:\n","                if r.area > 1:\n","                    box_coords = (r.bbox[1], r.bbox[0], r.bbox[3], r.bbox[2])\n","                    boxes.append(box_coords)\n","                    labels.append(_idx + 1)\n","                    # create a mask for one object, append to the list of masks\n","                    mask_obj = self.extract_single_mask(lab_mask, r.label)\n","                    masks.append(mask_obj)        \n","\n","        # convert everything into a torch.Tensor\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        masks = torch.as_tensor(masks, dtype=torch.uint8)\n","\n","        image_id = torch.tensor([idx])\n","\n","\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","\n","        iscrowd = torch.ones((len(masks),), dtype=torch.int64)\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            img, target = self.transforms(img, target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.idx_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fwuO5-af2cpW"},"source":["# prepare backbone\n","from torch import nn\n","from torchvision.models import resnet\n","from torchvision.models._utils import IntermediateLayerGetter\n","from collections import OrderedDict\n","from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n","\n","\n","class BackboneWithFPN(nn.Sequential):\n","    \"\"\"\n","    Adds a FPN on top of a model.\n","\n","    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to\n","    extract a submodel that returns the feature maps specified in return_layers.\n","    The same limitations of IntermediatLayerGetter apply here.\n","\n","    Arguments:\n","        backbone (nn.Module)\n","        return_layers (Dict[name, new_name]): a dict containing the names\n","            of the modules for which the activations will be returned as\n","            the key of the dict, and the value of the dict is the name\n","            of the returned activation (which the user can specify).\n","        in_channels_list (List[int]): number of channels for each feature map\n","            that is returned, in the order they are present in the OrderedDict\n","        out_channels (int): number of channels in the FPN.\n","\n","    Attributes:\n","        out_channels (int): the number of channels in the FPN\n","    \"\"\"\n","    def __init__(self, backbone, return_layers, in_channels_list, out_channels):\n","        body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n","        fpn = FeaturePyramidNetwork(\n","            in_channels_list=in_channels_list,\n","            out_channels=out_channels,\n","            # Alex: delete the additional block\n","            # FPN outputs a single layer\n","            #extra_blocks=LastLevelMaxPool(),\n","        )\n","        super(BackboneWithFPN, self).__init__(OrderedDict(\n","            [(\"body\", body), (\"fpn\", fpn)]))\n","        self.out_channels = out_channels\n","\n","def resnet_fpn_backbone(backbone_name, pretrained, out_ch, truncation):\n","\n","    \n","    backbone = resnet.__dict__[backbone_name](\n","        pretrained=pretrained,\n","        norm_layer=nn.BatchNorm2d)\n","    \n","    # Alex: the orginial implementation uses 4 outputs from the backbone\n","    # to replicate the best results in the paper, use resnet18 or resnet34\n","    # and truncation=1 or resnet50 and truncation=0 for the large model\n","    \n","    if backbone_name == 'resnet50':\n","       return_layers = {'layer1': 0, 'layer2': 1, 'layer3': 2, 'layer4': 3}\n","       in_channels_stage2 = 256\n","       in_channels_list = [\n","          in_channels_stage2,\n","          in_channels_stage2 * 2,\n","          in_channels_stage2 * 4,\n","          in_channels_stage2 * 8,\n","      ]\n","    elif backbone_name == 'resnet18' or backbone_name == 'resnet34':   \n","       if truncation == '0':\n","          return_layers = {'layer4':0}\n","       elif truncation == '1':\n","          return_layers = {'layer3':0}\n","       elif truncation == '2':\n","          return_layers = {'layer2':0}\n","       # Alex: I added the feature that returns the number of channels from the last layer in the net\n","       in_channels_list = [256]\n","       \n","    # These should be 256\n","    out_channels = out_ch\n","    \n","    return BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKaqulKnzext"},"source":["# Try to build a maskrcnn network as Alex\n","# Need to define backbone, rpn_anchor_generator, rpn_head, box_roi_pool, box_head, box_predictor\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.faster_rcnn import TwoMLPHead\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNHeads\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.mask_rcnn import MaskRCNN\n","\n","maskrcnn_args = {'min_size': 512, 'max_size': 1024, 'rpn_batch_size_per_image': 256, 'rpn_positive_fraction': 0.75,\n","                  'box_positive_fraction': 0.75, 'box_fg_iou_thresh': 0.75, 'box_bg_iou_thresh': 0.5,\n","                  'num_classes': None, 'box_batch_size_per_image': 256, 'rpn_nms_thresh': 0.75}\n","# rpn_anchor_generator\n","anchor_generator = AnchorGenerator(\n","    sizes=tuple([(2, 4, 8, 16, 32) for r in range(5)]),\n","    aspect_ratios=tuple([(0.1, 0.25, 0.5, 1, 1.5, 2) for rh in range(5)]))\n","\n","\n","# box_head\n","box_head = TwoMLPHead(in_channels=256*7*7,representation_size=128)\n","#Backbone->FPN->boxhead->boxpredictor\n","box_predictor = FastRCNNPredictor(in_channels=128, num_classes=3)\n","maskrcnn_heads = MaskRCNNHeads(in_channels=256, layers=(128,), dilation=1)\n","mask_predictor = MaskRCNNPredictor(in_channels=128, dim_reduced=128, num_classes=3)\n","\n","\n","\n","maskrcnn_args['box_head'] = box_head\n","maskrcnn_args['rpn_anchor_generator'] = anchor_generator\n","maskrcnn_args['mask_head'] = maskrcnn_heads\n","maskrcnn_args['mask_predictor'] = mask_predictor\n","maskrcnn_args['box_predictor'] = box_predictor\n","\n","backbone = resnet_fpn_backbone('resnet18', False, out_ch=256, truncation='1')\n","\n","model = MaskRCNN(backbone, 3, maskrcnn_args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c39E6G1HQDNi"},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # load an instance segmentation model pre-trained on COCO\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    # get number of input features for the classifier\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # replace the pre-trained head with a new one\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # now get the number of input features for the mask classifier\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # and replace the mask predictor with a new one\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeKSs9JJc4gc"},"source":["# import torchvision\n","# from torchvision.models.detection import FasterRCNN\n","# from torchvision.models.detection.rpn import AnchorGenerator\n","# from torchvision.models.detection import FasterRCNN\n","\n","# # load a pre-trained model for classification and return\n","# # only the features\n","# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","# # FasterRCNN needs to know the number of\n","# # output channels in a backbone. For mobilenet_v2, it's 1280\n","# # so we need to add it here\n","# backbone.out_channels = 1280\n","\n","# # let's make the RPN generate 5 x 3 anchors per spatial\n","# # location, with 5 different sizes and 3 different aspect\n","# # ratios. We have a Tuple[Tuple[int]] because each feature\n","# # map could potentially have different sizes and\n","# # aspect ratios\n","# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# # let's define what are the feature maps that we will\n","# # use to perform the region of interest cropping, as well as\n","# # the size of the crop after rescaling.\n","# # if your backbone returns a Tensor, featmap_names is expected to\n","# # be [0]. More generally, the backbone should return an\n","# # OrderedDict[Tensor], and in featmap_names you can choose which\n","# # feature maps to use.\n","# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","#                                                 output_size=7,\n","#                                                 sampling_ratio=2)\n","\n","# # put the pieces together inside a FasterRCNN model\n","# model = FasterRCNN(backbone,\n","#                    num_classes=3,\n","#                    rpn_anchor_generator=anchor_generator,\n","#                    box_roi_pool=roi_pooler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Bz7QCZ7QItQ"},"source":["import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.ToTensor())\n","    if train:\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":679},"id":"5fC8YKx3QJXU","executionInfo":{"status":"ok","timestamp":1635782122429,"user_tz":-480,"elapsed":228552,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"2d3ec538-c69a-4192-bc15-1146e21feb06"},"source":["!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio===0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n","Collecting torch==1.10.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1821.5 MB)\n","\u001b[K     |██████████████▋                 | 834.1 MB 1.6 MB/s eta 0:10:18tcmalloc: large alloc 1147494400 bytes == 0x555ea645e000 @  0x7f499be5d615 0x555e6cee64cc 0x555e6cfc647a 0x555e6cee92ed 0x555e6cfdae1d 0x555e6cf5ce99 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf5cd00 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6ceeb039 0x555e6cf2e409 0x555e6cee9c52 0x555e6cf5cc25 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58915 0x555e6ceeaafa 0x555e6cf58c0d 0x555e6cf579ee\n","\u001b[K     |██████████████████▌             | 1055.7 MB 1.4 MB/s eta 0:09:08tcmalloc: large alloc 1434370048 bytes == 0x555eeaab4000 @  0x7f499be5d615 0x555e6cee64cc 0x555e6cfc647a 0x555e6cee92ed 0x555e6cfdae1d 0x555e6cf5ce99 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf5cd00 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6ceeb039 0x555e6cf2e409 0x555e6cee9c52 0x555e6cf5cc25 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58915 0x555e6ceeaafa 0x555e6cf58c0d 0x555e6cf579ee\n","\u001b[K     |███████████████████████▌        | 1336.2 MB 1.4 MB/s eta 0:05:43tcmalloc: large alloc 1792966656 bytes == 0x555e6f8e6000 @  0x7f499be5d615 0x555e6cee64cc 0x555e6cfc647a 0x555e6cee92ed 0x555e6cfdae1d 0x555e6cf5ce99 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf5cd00 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6ceeb039 0x555e6cf2e409 0x555e6cee9c52 0x555e6cf5cc25 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58915 0x555e6ceeaafa 0x555e6cf58c0d 0x555e6cf579ee\n","\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.3 MB/s eta 0:01:40tcmalloc: large alloc 2241208320 bytes == 0x555eda6ce000 @  0x7f499be5d615 0x555e6cee64cc 0x555e6cfc647a 0x555e6cee92ed 0x555e6cfdae1d 0x555e6cf5ce99 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf5cd00 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6cfdbc66 0x555e6cf58daf 0x555e6ceeb039 0x555e6cf2e409 0x555e6cee9c52 0x555e6cf5cc25 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58915 0x555e6ceeaafa 0x555e6cf58c0d 0x555e6cf579ee\n","\u001b[K     |████████████████████████████████| 1821.5 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 1821458432 bytes == 0x555f60030000 @  0x7f499be5c1e7 0x555e6cf1c067 0x555e6cee64cc 0x555e6cfc647a 0x555e6cee92ed 0x555e6cfdae1d 0x555e6cf5ce99 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6ceeaafa 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee\n","tcmalloc: large alloc 2276827136 bytes == 0x555fcc944000 @  0x7f499be5d615 0x555e6cee64cc 0x555e6cfc647a 0x555e6cee92ed 0x555e6cfdae1d 0x555e6cf5ce99 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf58c0d 0x555e6ceeaafa 0x555e6cf58c0d 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeabda 0x555e6cf59737 0x555e6cf579ee 0x555e6ceeb271\n","\u001b[K     |████████████████████████████████| 1821.5 MB 5.8 kB/s \n","\u001b[?25hCollecting torchvision==0.11.1+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (24.6 MB)\n","\u001b[K     |████████████████████████████████| 24.6 MB 1.7 MB/s \n","\u001b[?25hCollecting torchaudio===0.10.0+cu113\n","  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.10.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 36.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu113) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu113) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.11.1+cu113) (7.1.2)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu111\n","    Uninstalling torch-1.9.0+cu111:\n","      Successfully uninstalled torch-1.9.0+cu111\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu111\n","    Uninstalling torchvision-0.10.0+cu111:\n","      Successfully uninstalled torchvision-0.10.0+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.10.0+cu113 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0+cu113 torchaudio-0.10.0+cu113 torchvision-0.11.1+cu113\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BeIQF-3ZQLgg","executionInfo":{"status":"ok","timestamp":1635782251151,"user_tz":-480,"elapsed":436,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"5dd2b826-a3f7-44bf-e634-127476022f80"},"source":["torch.cuda.get_device_name(0)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB'"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNMw0ZWOT8BT","executionInfo":{"status":"ok","timestamp":1635782252976,"user_tz":-480,"elapsed":385,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"86bb72e5-8d87-4caf-fc52-4766b0132855"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MaskRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=({'min_size': 512, 'max_size': 1024, 'rpn_batch_size_per_image': 256, 'rpn_positive_fraction': 0.75, 'box_positive_fraction': 0.75, 'box_fg_iou_thresh': 0.75, 'box_bg_iou_thresh': 0.5, 'num_classes': None, 'box_batch_size_per_image': 256, 'rpn_nms_thresh': 0.75, 'box_head': TwoMLPHead(\n","    (fc6): Linear(in_features=12544, out_features=128, bias=True)\n","    (fc7): Linear(in_features=128, out_features=128, bias=True)\n","  ), 'rpn_anchor_generator': AnchorGenerator(), 'mask_head': MaskRCNNHeads(\n","    (mask_fcn1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu1): ReLU(inplace=True)\n","  ), 'mask_predictor': MaskRCNNPredictor(\n","    (conv5_mask): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n","    (relu): ReLU(inplace=True)\n","    (mask_fcn_logits): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n","  ), 'box_predictor': FastRCNNPredictor(\n","    (cls_score): Linear(in_features=128, out_features=3, bias=True)\n","    (bbox_pred): Linear(in_features=128, out_features=12, bias=True)\n","  )},), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): BasicBlock(\n","          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): BasicBlock(\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n","    )\n","    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n","    (mask_head): MaskRCNNHeads(\n","      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu1): ReLU(inplace=True)\n","      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu2): ReLU(inplace=True)\n","      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu3): ReLU(inplace=True)\n","      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (relu4): ReLU(inplace=True)\n","    )\n","    (mask_predictor): MaskRCNNPredictor(\n","      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (relu): ReLU(inplace=True)\n","      (mask_fcn_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517},"id":"n9EHNaEwQNcz","executionInfo":{"status":"error","timestamp":1635784632465,"user_tz":-480,"elapsed":5138,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"9e430f25-6e73-4999-ceb9-96ff4e74c128"},"source":["from engine import train_one_epoch, evaluate\n","import utils\n","\n","# train on the GPU or on the CPU, if a GPU is not available\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 3\n","# use our dataset and defined transformations\n","dataset = Ct_seg_dataset('ct_lesion_seg', get_transform(train=True))\n","dataset_test = Ct_seg_dataset('ct_lesion_seg', get_transform(train=False))\n","\n","# split the dataset in train and test set\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","  dataset, batch_size=2, shuffle=True, num_workers=4,\n","  collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","  dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","  collate_fn=utils.collate_fn)\n","\n","# get the model using our helper function\n","# model = get_model_instance_segmentation(num_classes)\n","# Jeremy: model already defined\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                          momentum=0.9, weight_decay=0.0005)\n","# and a learning rate scheduler\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                              step_size=3,\n","                                              gamma=0.1)\n","\n","# let's train it for 100 epochs\n","num_epochs = 1\n","\n","\n","for epoch in range(num_epochs):\n","  # train for one epoch, printing every 10 iterations\n","  train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","  # update the learning rate\n","  lr_scheduler.step()\n","  # evaluate on the test dataset\n","  evaluate(model, data_loader_test, device=device)\n","\n","\n","torch.save(model.state_dict(),'ct_seg_model.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-cfc24d6f64d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/MyDrive/iss/PRS/PM/vision/references/detection/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[1;32m    111\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# FIXME assume for now that testing uses the largest scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"]}]},{"cell_type":"code","metadata":{"id":"CI9TuMg9QQh5"},"source":["# pick one image from the test set\n","img, target = dataset_test[2]\n"," \n","# put the model in evaluation mode\n","model.eval()\n","with torch.no_grad():\n","    prediction = model([img.to(device)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":185},"id":"vK2SCZ1vQUCO","executionInfo":{"status":"ok","timestamp":1635739943766,"user_tz":-480,"elapsed":1359,"user":{"displayName":"Jeremy Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03096517099448682819"}},"outputId":"2ca9ec47-2ffc-4f43-b966-70d9e49adc70"},"source":["import matplotlib.pyplot as plt\n","import cv2\n","\n","fig, ax = plt.subplots(1, 3)\n","ax[0].imshow(img.mul(255).permute(1, 2, 0).byte().numpy(), cmap='gray')\n","ax[1].imshow(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy(), cmap='gray')\n","mask = target['masks'].mul(255).permute(1, 2, 0).byte().numpy()\n","print(mask.shape)\n","mask1 = mask[:,:,0]\n","for i in range(1, mask.shape[2]):\n","  mask1 = mask1 + mask[:,:,i]\n","mask = cv2.cvtColor(mask1, cv2.COLOR_GRAY2RGB)\n","ax[2].imshow(mask, cmap='gray')\n","print(target['image_id'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(512, 512, 1)\n","tensor([58])\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 3 Axes>"]},"metadata":{"needs_background":"light"}}]}]}